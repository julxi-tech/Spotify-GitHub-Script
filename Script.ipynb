{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Streaming Data\n",
    "\n",
    "First you have to download both your normal streaming data and your extended streaming data on Spotify:\n",
    "\n",
    "    -> Go to your account information\n",
    "\n",
    "    -> Go to Privacy Settings\n",
    "    \n",
    "    -> Request account data and extended streaming history\n",
    "\n",
    "After you got the data, copy the files in the directory \"Data/DataFromSpotify\"\n",
    ". With that all the paths, should work just fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preperations\n",
    "\n",
    "Importing all the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important all the needed libraries\n",
    "import pandas as pd \n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the Spotify API Client:\n",
    "\n",
    "    -> Go to the Spotify Developer Site\n",
    "\n",
    "    -> Create a new App\n",
    "\n",
    "    -> Retrieve the needed Keys and insert them below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "clientID = \"Your Client ID\"\n",
    "\n",
    "secretID = \"Cour Secret ID\"\n",
    "\n",
    "sp = spotipy.Spotify(client_credentials_manager=SpotifyClientCredentials(client_id=clientID, client_secret=secretID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the streaming Data\n",
    "\n",
    "    -> Put all the streaming history files (the ones starting with Streaming_History_Audio) in the directory \"Data/Streaming_History_Files\" \n",
    "    \n",
    "    -> The following code goes through all the files and merges them \n",
    "\n",
    "    -> It wil also create a new csv file called merged_streaming_history.csv which is stored in the data directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON files merged and converted to CSV successfully!\n"
     ]
    }
   ],
   "source": [
    "# List the paths of your JSON files\n",
    "streamingHistoryFilesDirectory = os.fsencode('Data/Streaming_History_Files')\n",
    "\n",
    "# Define the path for the output CSV file\n",
    "output_csv_file = 'Data/RawData/merged_streaming_history.csv' \n",
    "\n",
    "# List to store all data from JSON files\n",
    "all_data = []\n",
    "\n",
    "# Loop through each JSON file and extract data\n",
    "for json_file in os.listdir(streamingHistoryFilesDirectory):\n",
    "    file_path = os.path.join(streamingHistoryFilesDirectory, json_file)\n",
    "    if os.path.isfile(file_path) and json_file.decode('utf-8').endswith('.json'):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            all_data.extend(data)\n",
    "\n",
    "# Check if there is any data to write to CSV\n",
    "if not all_data:\n",
    "    print(\"No data found in JSON files.\")\n",
    "else:\n",
    "    # Determine the keys for CSV header (assuming each JSON object has the same structure)\n",
    "    csv_header = all_data[0].keys()\n",
    "\n",
    "    # Write data to CSV file\n",
    "    with open(output_csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        csv_writer = csv.DictWriter(f, fieldnames=csv_header)\n",
    "\n",
    "        # Write the CSV header\n",
    "        csv_writer.writeheader()\n",
    "\n",
    "        # Write each JSON object as a row in the CSV file\n",
    "        csv_writer.writerows(all_data)\n",
    "\n",
    "    print(\"JSON files merged and converted to CSV successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pandas data frame for this data Set and clean the data:\n",
    "\n",
    "    -> We want to have only tracks and no podcasts in this data set, we can also dropt the episode_name, episode_show_name and spotify_episode_uri column\n",
    "\n",
    "    -> In this data set we have the Spotify URI, we want to add a column to have the song ID which is the second part of the Spotify URI (after the 'track:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vz/zlsdllt500lgf0_qrzxkk3xw0000gn/T/ipykernel_14950/3368755055.py:2: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  mergedData_df = pd.read_csv('Data/RawData/merged_streaming_history.csv')\n"
     ]
    }
   ],
   "source": [
    "# Create the Dataframe\n",
    "mergedData_df = pd.read_csv('Data/RawData/merged_streaming_history.csv')\n",
    "\n",
    "# Drop the rows where the spotify_episode_uri has any values\n",
    "mergedData_df = mergedData_df[mergedData_df['spotify_episode_uri'].isna()]\n",
    "\n",
    "# Drop the columns we don't need anymore\n",
    "mergedData_df.drop(['episode_name', 'episode_show_name', 'spotify_episode_uri'], axis=1, inplace=True)\n",
    "\n",
    "# Create the songID column\n",
    "mergedData_df['songID'] = mergedData_df['spotify_track_uri']\n",
    "mergedData_df['songID'] = mergedData_df['songID'].str[-22:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Master Data \n",
    "\n",
    "The Master Data is the Dataframe that we will add all the information to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So far the master data is the merged data \n",
    "masterData_df = mergedData_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Data \n",
    "\n",
    "Append the information if the song is saved in my library.\n",
    "\n",
    "    -> Load the library songs as a dataframe\n",
    "\n",
    "    -> Loop through the streaming data and check if the song is in the library\n",
    "\n",
    "    -> Add a column \"in_library\" with values True and False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/DataFromSpotify/YourLibrary.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "library_df = pd.DataFrame(data['tracks'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "addedColumn = []\n",
    "\n",
    "for trackName in masterData_df['master_metadata_track_name']:\n",
    "    if trackName in set(library_df['track']): \n",
    "        addedColumn.append(True)\n",
    "    else:\n",
    "        addedColumn.append(False)\n",
    "\n",
    "masterData_df['In Favourites'] = addedColumn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track Data\n",
    "\n",
    "Now we perform some API requests to collect some more information about each song. We use the Spotify API to do that. \n",
    "\n",
    "We want to get the following information:\n",
    "\n",
    "    -> Release Dat \n",
    "    -> Genre\n",
    "    -> Popularity of the artist\n",
    "    -> Popularity of song\n",
    "    -> Is the song explicit?\n",
    "    -> Id of song\n",
    "    -> Name of song\n",
    "    -> popularity of song\n",
    "\n",
    "Please refer to this https://developer.spotify.com/documentation/web-api/reference/get-several-tracks for an explanation for every data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get information about 50 songs in one request. \n",
    "\n",
    "Therfore we make a list of lists, where each list contains 50 songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a unique list of URIs\n",
    "uniqueURIs = list(set(masterData_df['spotify_track_uri']))\n",
    "\n",
    "# Aggregate those ids in a list of lists -> 50 ids in one list\n",
    "listOfIDlists = []\n",
    "listOfIDs = []\n",
    "for index, uri in enumerate(uniqueURIs):\n",
    "    listOfIDs.append(uri)\n",
    "    if index % 49 == 0 or index == len(uniqueURIs) -1:\n",
    "        if index != 0:\n",
    "            listOfIDlists.append(listOfIDs)\n",
    "            listOfIDs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we loop through listOfIDlists and make an API call for each. \n",
    "\n",
    "We store the reponses in a dedicated json file. Path: /Data/RawData/songInformation.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "badReponses = []\n",
    "for index, uriList in enumerate(listOfIDlists):\n",
    "    # ToDo Delete this line!\n",
    "    if index < 2:\n",
    "        try:\n",
    "            response = sp.tracks(uriList)\n",
    "            time.sleep(10)\n",
    "            responses.append(response)\n",
    "        except Exception as e:\n",
    "            badReponses.append(uriList)\n",
    "            print(e)\n",
    "\n",
    "# Create a json file to store the responses\n",
    "with open('Data/RawData/songInformation.json', 'w') as json_file:\n",
    "    json.dump(responses, json_file, indent=4)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the information, but we need to extract the relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have the reponses and want to extract the data an convert it to a dataframe\n",
    "listOfRows = []\n",
    "counter = 0\n",
    "for trackList in responses:\n",
    "    for track in trackList['tracks']:\n",
    "        songID = track['id']\n",
    "        songName = track['name']\n",
    "        try:\n",
    "            songDuration = track['duration_ms']\n",
    "        except KeyError:\n",
    "            songDuration = 1\n",
    "        try:\n",
    "            songPopularity = track['popularity']\n",
    "        except KeyError:\n",
    "            songPopularity = 101\n",
    "        try:\n",
    "            explicit = track['explicit']\n",
    "        except KeyError:\n",
    "            explicit = False\n",
    "        try:\n",
    "            releaseDate = track['album']['release_date'][0:4]\n",
    "        except KeyError:\n",
    "            releaseDate = '9999'\n",
    "        try:\n",
    "            if len(track['artists']) > 0:\n",
    "                artist_name = track['artists'][0]['name']\n",
    "                artist_uri = track['artists'][0]['uri']\n",
    "            else:\n",
    "                artist_name = 'Artist Not Found'\n",
    "                artist_uri = 'Artist Not Found'\n",
    "        except KeyError:\n",
    "            artist = 'Artist Not Found'\n",
    "            artist_uri = 'Artist Not Found'\n",
    "        dictToAdd = {\n",
    "            'songID': songID,\n",
    "            'name': songName,\n",
    "            'duration_ms': songDuration,\n",
    "            'song_popularity': songPopularity,\n",
    "            'song_explicit': explicit,\n",
    "            'release_Date': releaseDate,\n",
    "            'first_artist': artist_name,\n",
    "            'artist_uri': artist_uri,\n",
    "            #'genreOfArtist': genreOfArtis\n",
    "        }\n",
    "        listOfRows.append(dictToAdd)\n",
    "        counter = counter + 1\n",
    "\n",
    "df_songData = pd.DataFrame(listOfRows)\n",
    "\n",
    "# Store the extracted data frame as a single csv file to store it permanenetly\n",
    "df_songData.to_csv('Data/RawData/songInformation_extracted.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track Analysis\n",
    "\n",
    "In the following we will get data of an audio analysis of each track. We do this by making another Spotify API request. We will get the following information for each song:\n",
    "    -> danceability\n",
    "    -> energy\n",
    "    -> key\n",
    "    -> loudness\n",
    "    -> mode\n",
    "    -> speechiness\n",
    "    -> acousticness\n",
    "    -> instrumentalness\n",
    "    -> liveness\n",
    "    -> valence\n",
    "    -> tempo\n",
    "    -> type\n",
    "    -> id\n",
    "    -> uri\n",
    "    -> track_href\n",
    "    -> analysis_url\n",
    "    -> duration_ms\n",
    "    -> time_signature\n",
    "\n",
    "Please refer to https://developer.spotify.com/documentation/web-api/reference/get-several-audio-features for an explanation of every data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the audio analysis for 100 songs in one request. So we take the same approach as before, but now we make a list of lists, where each inner list contains 100 songs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate those ids in a list of lists -> 100 ids in one list\n",
    "listOfIDlists_audioAnalysis = []\n",
    "listOfIDs_audioAnalysis = []\n",
    "for index, uri in enumerate(uniqueURIs):\n",
    "    listOfIDs_audioAnalysis.append(uri)\n",
    "    if index % 99 == 0 or index == len(uniqueURIs) -1:\n",
    "        if index != 0:\n",
    "            listOfIDlists_audioAnalysis.append(listOfIDs_audioAnalysis)\n",
    "            listOfIDs_audioAnalysis = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we loop through listOfIDlists_audioAnalysis and make an API call for each. \n",
    "\n",
    "We store the reponses in a dedicated json file. Path: /Data/RawData/song_audioAnalysis.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_audioAnalysis = []\n",
    "badReponses_audioAnalysis = []\n",
    "for index, uriList in enumerate(listOfIDlists_audioAnalysis):\n",
    "    # ToDo Delete this line!\n",
    "    if index < 2:\n",
    "        try:\n",
    "            response_audioAnalysis = sp.audio_features(tracks = uriList)\n",
    "            time.sleep(10)\n",
    "            responses_audioAnalysis.append(response_audioAnalysis)\n",
    "        except Exception as e:\n",
    "            badReponses_audioAnalysis.append(uriList)\n",
    "            print(e)\n",
    "\n",
    "with open('Data/RawData/song_audioAnalysis.json', 'w') as json_file:\n",
    "    json.dump(responses_audioAnalysis, json_file, indent=4)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfDataFrames_audioAnalysis = []\n",
    "for response_forList in responses_audioAnalysis:\n",
    "    listOfDataFrames_audioAnalysis.append(pd.DataFrame(response_forList))\n",
    "\n",
    "df_songAnalysis = pd.concat(listOfDataFrames_audioAnalysis, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together\n",
    "\n",
    "Now we have all the information and now we just have to join the dataframes. With data sets from the API calls we will do a left join on the songID where the masterData is the left part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge 1\n",
    "\n",
    "We will append the information for each track to the master data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "commonKey = 'songID'\n",
    "masterData_df = pd.merge(masterData_df, df_songData, on=commonKey, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge 2\n",
    "\n",
    "In this merge we will append the informaiton of the audio analysis of each song to the master data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First rename the id column of the audio analysis to songID\n",
    "df_songAnalysis['songID'] = df_songAnalysis['id']\n",
    "df_songAnalysis.drop(['id'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "commonKey = 'songID'\n",
    "masterData_df = pd.merge(masterData_df, df_songAnalysis, on=commonKey, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finished\n",
    "\n",
    "Now we export the master data as a csv file and we can analyse and visualize it in Looker Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterData_df.to_csv('MasterData.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
